<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Jin Zhu" />


<title>An Introduction to abess</title>

<script src="abess-guide_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="abess-guide_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="abess-guide_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="abess-guide_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="abess-guide_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="abess-guide_files/navigation-1.1/tabsets.js"></script>
<link href="abess-guide_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="abess-guide_files/highlightjs-9.12.0/highlight.js"></script>
<link href="abess-guide_files/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="abess-guide_files/anchor-sections-1.0/anchor-sections.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">An Introduction to <code>abess</code></h1>
<h4 class="author">Jin Zhu</h4>

</div>

<div id="TOC">
<ul>
<li><a href="#briey-introduction">Briey introduction</a></li>
<li><a href="#quick-example">Quick example</a><ul>
<li><a href="#fixed-support-size-best-subset-selection">Fixed support size best subset selection</a></li>
<li><a href="#adaptive-best-subset-selection">Adaptive best subset selection</a></li>
</ul></li>
<li><a href="#advanced-features">Advanced features</a><ul>
<li><a href="#feature-screening-for-ultra-high-dimensional-dataset">Feature screening for ultra-high dimensional dataset</a></li>
</ul></li>
</ul>
</div>

<div id="briey-introduction" class="section level2">
<h2>Briey introduction</h2>
<p>The R package <code>abess</code> implement a polynomial algorithm in the <a href="https://www.pnas.org/content/117/52/33117"></a> for best-subset selection problem: <span class="math display">\[\min_{\beta \in \mathbb{R}^p} \frac{1}{2n} \| y - X\beta\|_2^2, \text{ subject to } \|\beta\|_0 \leq s,\]</span> where <span class="math inline">\(\| \cdot \|_2\)</span> is the <span class="math inline">\(\ell_2\)</span> norm, <span class="math inline">\(\|\beta\|_0=\sum_{i=1}^pI( \beta_i\neq 0)\)</span> is the <span class="math inline">\(\ell_0\)</span> norm of <span class="math inline">\(\beta\)</span>, and the sparsity level <span class="math inline">\(s\)</span> is usually an unknown non-negative integer. Next, we present an example to show how to use the <strong>abess</strong> package to solve a simple problem.</p>
</div>
<div id="quick-example" class="section level2">
<h2>Quick example</h2>
<div id="fixed-support-size-best-subset-selection" class="section level3">
<h3>Fixed support size best subset selection</h3>
<p>We generate a design matrix <span class="math inline">\(X\)</span> containing 300 observation and each observation has 1000 predictors. The response variable <span class="math inline">\(y\)</span> is linearly related to the first, second, and fifth predictors in <span class="math inline">\(X\)</span>: <span class="math display">\[y = 3X_1 + 1.5X_2 + 2X_5 + \epsilon,\]</span> where <span class="math inline">\(\epsilon\)</span> is a standard normal random variable.</p>
<pre class="r"><code>library(abess)
synthetic_data &lt;- generate.data(n = 300, p = 1000, 
                                beta = c(3, 1.5, 0, 0, 2, rep(0, 995)))
dim(synthetic_data[[&quot;x&quot;]])</code></pre>
<pre><code>## [1]  300 1000</code></pre>
<pre class="r"><code>head(synthetic_data[[&quot;y&quot;]])</code></pre>
<pre><code>##           [,1]
## [1,] -4.063922
## [2,]  3.855246
## [3,] -3.041391
## [4,] -1.081257
## [5,]  4.986772
## [6,]  4.470901</code></pre>
<pre class="r"><code>dat &lt;- cbind.data.frame(&quot;y&quot; = synthetic_data[[&quot;y&quot;]], 
                        synthetic_data[[&quot;x&quot;]])</code></pre>
<p>Then, we use the main function <code>abess</code> in the package to fit this dataset. By setting the arguments <code>support.size = s</code>, <code>abess</code> function conducts <strong>Algorithm 1</strong> in the <a href="https://www.pnas.org/content/117/52/33117"></a> for best-subset selection with a sparsity level <code>s</code>. In our example, we set the options: <code>support.size = 3</code>, and we run <strong>Algorithm 1</strong> with the following command:</p>
<pre class="r"><code>abess_fit &lt;- abess(y ~ ., data = dat, support.size = 3)</code></pre>
<p>The output of <code>abess</code> comprises the selected best model:</p>
<pre class="r"><code>head(coef(abess_fit, sparse = FALSE))</code></pre>
<pre><code>##                       3
## (intercept) -0.01802179
## x1           2.96418205
## x2           1.45090693
## x3           0.00000000
## x4           0.00000000
## x5           1.90592036</code></pre>
<p>The best model’s support set is identical to the ground truth, and the coefficient estimation is the same as the oracle estimator given by <code>lm</code> function:</p>
<pre class="r"><code>lm(y ~ ., data = dat[, c(1, c(1, 2, 5) + 1)])</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ ., data = dat[, c(1, c(1, 2, 5) + 1)])
## 
## Coefficients:
## (Intercept)           x1           x2           x5  
##    -0.01802      2.96418      1.45091      1.90592</code></pre>
<!-- Users could `print`, `summary` or `predict` this bestmodel object just like working with classical regression modeling. This would be helpful for data scientists who are familiar with `lm` functions in R. -->
</div>
<div id="adaptive-best-subset-selection" class="section level3">
<h3>Adaptive best subset selection</h3>
<p>Imaging we are unknown about the true sparsity level in real world data, and thus, we need to determine the most proper one. The <strong>Algorithm 3</strong> in the <a href="https://www.pnas.org/content/117/52/33117"></a> is designed for this scenario. <code>abess</code> is capable of performing this algorithm:</p>
<pre class="r"><code>abess_fit &lt;- abess(y ~ ., data = dat)</code></pre>
<p>The output of <code>abess</code> also comprises the selected best model:</p>
<pre class="r"><code>best_size &lt;- abess_fit[[&quot;best.size&quot;]]
print(best_size)</code></pre>
<pre><code>## [1] 3</code></pre>
<pre class="r"><code>head(coef(abess_fit, support.size = best_size, sparse = FALSE))</code></pre>
<pre><code>##                       3
## (intercept) -0.01802179
## x1           2.96418205
## x2           1.45090693
## x3           0.00000000
## x4           0.00000000
## x5           1.90592036</code></pre>
<p>The output model accurately detect the true model size, which implies the <strong>Algorithm 3</strong> efficiently find both the optimal sparsity level and true effective predictors.</p>
</div>
</div>
<div id="advanced-features" class="section level2">
<h2>Advanced features</h2>
<div id="feature-screening-for-ultra-high-dimensional-dataset" class="section level3">
<h3>Feature screening for ultra-high dimensional dataset</h3>
<p>The <a href="https://archive.ics.uci.edu/ml/datasets/communities+and+crime"></a> consists of 18 variables about crime from the 1995 FBI UCR (e.g., per capita arson crimes and per capita violent crimes), communities information in the U.S. (e.g., the percent of the population considered urban), socio-economic data from the 90s census (e.g., the median family income), and law enforcement data from the 1990 law enforcement management and admin stats survey (e.g., per capita number of police officers). It would be appropriate if any of the crime state in community can be modeled by the basic community information, socio-economic and law enforcement state in community. Here, without the loss of generality, per capita violent crimes is chosen as the response variable, and 102 numerical variables as well as their pairwise interactions is considered as predictors. <!-- Note that, the numerical variables with at least 50\% percentages missing observations are excluded,  --> <!-- and 200 observations without missing records are randomly picked out from the pool.  --> The pre-processed dataset for statistical modeling has 200 observations and 5253 predictors, and the code for pre-processing are openly shared in <a href="https://github.com/abess-team/abess/blob/master/R-package/data-raw/DATASET_VIGNETTES.R"></a>.<br />
The pre-processed dataset can be freely downloaded by running:</p>
<pre class="r"><code>working_directory &lt;- getwd()
if (file.exists(&quot;crime.rda&quot;)) {
  load(&quot;crime.rda&quot;)
} else {
  crime_data_url &lt;- &quot;https://github.com/abess-team/abess/raw/master/R-package/data-raw/crime.rda&quot;
  download.file(crime_data_url, &quot;crime.rda&quot;)
  load(file.path(working_directory, &quot;crime.rda&quot;))
}</code></pre>
<p>As mentioned before, this dataset comprises 5000+ features, much larger than the number of observations:</p>
<pre class="r"><code>dim(crime)</code></pre>
<pre><code>## [1]  500 5254</code></pre>
<p>And thus, it would be better to first perform feature screening, which is also supported by the <code>abess</code> function. Suppose we are interested in retaining 1000 variables with the largest marginal utility, then we can conduct the command:</p>
<pre class="r"><code>abess_fit &lt;- abess(y ~ ., data = crime, screening.num = 1000)
abess_fit</code></pre>
<pre><code>## Call:
## abess.formula(formula = y ~ ., data = crime, screening.num = 1000)
## 
##    support.size       dev      GIC
## 1             0 381863.43 6426.409
## 2             1 172846.92 6042.701
## 3             2 155974.89 6003.965
## 4             3 147047.98 5987.116
## 5             4 141513.44 5980.554
## 6             5 135523.59 5971.549
## 7             6 132507.52 5972.916
## 8             7 128890.08 5971.696
## 9             8 123742.00 5963.935
## 10            9 120870.35 5964.815
## 11           10 118266.27 5966.545
## 12           11 116297.21 5970.770
## 13           12 113114.92 5969.517
## 14           13 110998.98 5972.696
## 15           14 109047.24 5976.445
## 16           15 108012.14 5984.296
## 17           16 106262.19 5988.749
## 18           17 105408.98 5997.338
## 19           18 104432.72 6005.306
## 20           19 103561.37 6013.736
## 21           20 103045.98 6023.861
## 22           21 102150.55 6032.117
## 23           22 101433.48 6041.215
## 24           23 100600.07 6049.709
## 25           24  97994.13 6049.207
## 26           25  97298.40 6058.264
## 27           26  96514.02 6066.836
## 28           27  95857.94 6076.046
## 29           28  95100.64 6084.700
## 30           29  94671.51 6095.058
## 31           30  93423.23 6101.042
## 32           31  92217.23 6107.165
## 33           32  91889.75 6118.006</code></pre>
<p>The returned object of <code>abess</code> includes the features selected by screening. We exhibit six variables of them:</p>
<pre class="r"><code>head(abess_fit[[&quot;screening.vars&quot;]])</code></pre>
<pre><code>## [1] &quot;pctBlack&quot;     &quot;pctWhite&quot;     &quot;medIncome&quot;    &quot;pctWdiv&quot;      &quot;pctPubAsst&quot;  
## [6] &quot;medFamIncome&quot;</code></pre>
<p>Then, by the generic <code>extract</code> function, we can obtain the best model detected by <code>ABESS</code> algorithm, and get the variables in the best model:</p>
<pre class="r"><code>best_model &lt;- extract(abess_fit)
str(best_model)</code></pre>
<pre><code>## List of 7
##  $ beta        :Formal class &#39;dgCMatrix&#39; [package &quot;Matrix&quot;] with 6 slots
##   .. ..@ i       : int [1:8] 303 331 368 442 1747 3014 3267 3950
##   .. ..@ p       : int [1:2] 0 8
##   .. ..@ Dim     : int [1:2] 5253 1
##   .. ..@ Dimnames:List of 2
##   .. .. ..$ : chr [1:5253] &quot;pop&quot; &quot;perHoush&quot; &quot;pctBlack&quot; &quot;pctWhite&quot; ...
##   .. .. ..$ : chr &quot;8&quot;
##   .. ..@ x       : num [1:8] 0.307252 -0.457335 0.537162 -0.07251 0.000439 ...
##   .. ..@ factors : list()
##  $ intercept   : num 530
##  $ support.size: int 8
##  $ support.vars: chr [1:8] &quot;pctBlack:pctWhite&quot; &quot;pctBlack:pctCollGrad&quot; &quot;pctBlack:pctPopDenseHous&quot; &quot;pctWhite:pctKids2Par&quot; ...
##  $ support.beta: num [1:8] 0.307252 -0.457335 0.537162 -0.07251 0.000439 ...
##  $ dev         : num 123742
##  $ tune.value  : num 5964</code></pre>
<pre class="r"><code>best_vars &lt;- best_model[[&quot;support.vars&quot;]]
best_vars</code></pre>
<pre><code>## [1] &quot;pctBlack:pctWhite&quot;                   
## [2] &quot;pctBlack:pctCollGrad&quot;                
## [3] &quot;pctBlack:pctPopDenseHous&quot;            
## [4] &quot;pctWhite:pctKids2Par&quot;                
## [5] &quot;pctPubAsst:ownHousQrange&quot;            
## [6] &quot;pctEmployMfg:pctVacantBoarded&quot;       
## [7] &quot;pctMaleDivorc:pctSmallHousUnits&quot;     
## [8] &quot;pctKidsBornNevrMarr:pctVacantBoarded&quot;</code></pre>
<!-- From the linear model based on the selected features, we see that the two predictors,  -->
<!-- 'pctMaleDivorc:pctKidsBornNevrMarr'  -->
<!-- (i.e., the linear interaction between the percentage of divorced males and the percentage of kids born to never married)  -->
<!-- and 'pct65up:pctPopDenseHous'  -->
<!-- (i.e., the linear interaction between the percentage of population that is 65 at least in age and the percent of persons in dense housing that have more than 1 person per room), have the most significant impact on the response:  -->
<!-- We visualize the relationship between the response and the  -->
<!-- two most significant -->
<!-- selected predictors: -->
<!-- ![](./crime.jpg) -->
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
